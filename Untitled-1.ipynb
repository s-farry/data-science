{
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Introduction\n",
                "\n",
                "I'll focus on developing a regression model to determine the interest rate int_rate from the available data. I outline in this notebook a three stage approach based on feature identification and selection, model selection and hyperparameter tuning"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "First some standard imports"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "#pandas, numpy and math for managing and manipulating our data\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import math\n",
                "\n",
                "#matplotlib for data visualisation\n",
                "import matplotlib\n",
                "import matplotlib.pyplot as plt"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "First I load the dataset and then drop any duplicates there might be. The customer ID is set to be the index or unique identifier of the dataset"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "dataframe = pd.read_csv('DataScienceAssignment/DataScienceAssignment.csv')\n",
                "dataframe.drop_duplicates(inplace=True)\n",
                "dataframe.set_index('id',inplace=True)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Get some info on the dataframe"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "print('The total number of nan entries is %i'%(dataframe.isna().sum().sum()))\n",
                "print('The columns with the most missing entries')\n",
                "print(dataframe.isna().sum().sort_values()[::-1][0:10])"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "I split our dataset into features for predicting, and the true value of the interest rate (grade is dropped for now, could be done similarly)"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "X = dataframe.drop(labels=['int_rate','grade'],axis=1)\n",
                "y = dataframe['int_rate']\n",
                "\n",
                "# Define an array to keep track of any columns which should be dropped\n",
                "columns_to_drop = []"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Feature Importance\n",
                "Here I can run a mutual info regression algorithm on the numerical values to get an idea of the feature importance."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "from sklearn.feature_selection import mutual_info_regression\n",
                "X_num = X.select_dtypes(include=np.number).fillna(0)\n",
                "X_cat = X.select_dtypes(['object']).fillna('None')\n",
                "print('There are %i numerical features and %i categorical categories'%(X_num.shape[1], X_cat.shape[1]))\n",
                "\n",
                "num_feature_importances=mutual_info_regression(X_num,y)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Plot the feature importance for all, then the highest and lowest"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "fig,(ax1,ax2,ax3) = plt.subplots(1,3, figsize = (30,10))\n",
                "\n",
                "#define some lo and hi thresholds\n",
                "lo_threshold, hi_threshold = 0.02, 0.3\n",
                "\n",
                "#make three dataframes, all, hi and lo based on importance thresholds\n",
                "feature_importance_df = pd.DataFrame.from_dict({'feature' : X_num.columns.values, 'importance' : num_feature_importances})\n",
                "feature_importance_df_hi = feature_importance_df[feature_importance_df['importance'] > hi_threshold]\n",
                "feature_importance_df_lo = feature_importance_df[feature_importance_df['importance'] < lo_threshold]\n",
                "\n",
                "# draw bar plots\n",
                "ax1.barh(range(len(feature_importance_df))   , feature_importance_df['importance'])\n",
                "ax2.barh(range(len(feature_importance_df_hi)), feature_importance_df_hi['importance'], tick_label = feature_importance_df_hi['feature'])\n",
                "ax3.barh(range(len(feature_importance_df_lo)), feature_importance_df_lo['importance'], tick_label = feature_importance_df_lo['feature'])\n",
                "\n",
                "#some lines for indication of the thresholds\n",
                "ax1.axvline(x=lo_threshold, color = 'red', linestyle='--')\n",
                "ax1.axvline(x=hi_threshold, color = 'red', linestyle='--')\n",
                "\n",
                "# labels\n",
                "ax1.set_ylabel('Feature Index')\n",
                "ax1.set_xlabel('Importance')\n",
                "ax2.set_xlabel('Importance')\n",
                "ax3.set_xlabel('Importance')\n",
                "\n",
                "#titles\n",
                "ax1.set_title('All Features')\n",
                "ax2.set_title('Features above %.2f importance'%hi_threshold)\n",
                "ax3.set_title('Features below %.2f importance'%lo_threshold)\n",
                "\n",
                "#and show\n",
                "plt.show()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Let's illustrate these correlations visually"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "fig0, ax0 = plt.subplots()\n",
                "ax0.hist(y, bins = 100)\n",
                "ax0.set_ylabel('Occurences')\n",
                "ax0.set_xlabel('Interest Rate [%]')\n",
                "plt.show()\n",
                "\n",
                "fig,(ax1,ax2,ax3) = plt.subplots(1,3, sharey=True, figsize = (30,10))\n",
                "ax1.hist2d(X['installment'], y, bins = 100, density = False, cmin = 5)\n",
                "ax2.hist2d(X['last_pymnt_amnt'],y, bins = 100, cmin = 5)\n",
                "ax3.hist2d(X['collection_recovery_fee'],y, bins = 100, cmin = 5)\n",
                "ax1.set_ylabel('Interest Rate [%]')\n",
                "ax1.set_xlabel('Installment')\n",
                "ax2.set_xlabel('Last Payment Amount')\n",
                "ax3.set_xlabel('Collection Recovery Fee')\n",
                "\n",
                "plt.show()\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Clear that although we see patterns, there isn't clear relationships we can benefit from, which indicates a simple model won't perform so well. Let's mark those numerical columns with low feature importance for removal. Note than Unnamed: 0 has a high importance, but probably for some other reason related to the data order, so including it will probably result in a large variance when we move to the test sample. Let's remove it."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "columns_to_drop += feature_importance_df_lo['feature'].values.tolist()\n",
                "columns_to_drop += [ 'Unnamed: 0']"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Here we convert the columns containing dates into the right format (this will allow us to use timestamps in the regression algorithm)"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Sci-kit Learn Pipelines for Processing, Training and Predicting\n",
                "Now that I have loaded our dataframe and identified the features, 'll make heavy use of pipelines in Scikit Learn to transform the data from its original form, perform the training, and predict the final results of any dataset. There are a number of advantages to this\n",
                "1. Reproducibility\n",
                "2. Ease of deployment\n",
                "3. Ensure no data leakage"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "from sklearn.naive_bayes import GaussianNB\n",
                "from sklearn.base import TransformerMixin\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.model_selection import train_test_split, GridSearchCV\n",
                "from sklearn.linear_model import LinearRegression, Lasso\n",
                "from sklearn.svm import SVR\n",
                "from sklearn.decomposition import PCA\n",
                "from sklearn.cross_decomposition import PLSRegression\n",
                "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
                "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
                "from sklearn.ensemble import HistGradientBoostingRegressor, GradientBoostingRegressor\n",
                "from sklearn.pipeline import make_pipeline, Pipeline\n",
                "from sklearn.compose import make_column_transformer, ColumnTransformer\n",
                "from sklearn.compose import make_column_selector\n",
                "from sklearn.impute import SimpleImputer\n",
                "from sklearn.metrics import mean_squared_error, r2_score\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.neighbors import KNeighborsRegressor\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "First, a couple of custom functions for formatting the date and also reducing the number of unique categories of employee title"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "class DateFormatter(TransformerMixin):\n",
                "\n",
                "    def fit(self, X, y=None):\n",
                "        # stateless transformer\n",
                "        return self\n",
                "\n",
                "    def transform(self, X):\n",
                "        # assumes X is a DataFrame\n",
                "        Xdate = X.apply(pd.to_datetime).apply(pd.to_numeric)\n",
                "        return Xdate\n",
                "\n",
                "class EmpTitleImputer(TransformerMixin):\n",
                "    def fit(self, X, y = None):\n",
                "        return self\n",
                "\n",
                "    def transform(self, X):\n",
                "        counts = X[X.columns[0]].value_counts().to_frame()\n",
                "        common = counts[counts[X.columns[0]] > 50].index.to_list()\n",
                "        X[X.columns[0]] = np.where(X[X.columns[0]].isin(common)<50, 'None', X[X.columns[0]])\n",
                "        return X"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "The first step is to set up a preprocessing pipeline. There are four main steps involved\n",
                "* Convert all date fields to a date time format\n",
                "* Numeric features corresponding to \"months since\" are converted to a large value (as no information here indicates inf. and not 0)\n",
                "* Other numeric features are filled with the mean if not available\n",
                "* Categorical features with a relation (e.g. sub grade) are converted to Ordinals to maintain information about the hierarchy\n",
                "* The emp_title field is adjusted so that only entries used over 50 times are considered a category, others are all grouped together (Note that there are 3300 unique emp titles in 75000 entries, so this greatly reduces the number of features). This is performed by the EmpTitleImputer custom function above\n",
                "* All other non-numerical categories are One Hot Encodeed\n",
                "* Some columns are dropped which shouldn't make their way into the final training as they are either all the same or don't provide useful information\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# features which we don't want to use in the training\n",
                "columns_to_drop += ['desc','title']\n",
                "\n",
                "# any date features, which will be onverted to ordinals so they can be interpreted as numbers\n",
                "date_features  = ['issue_d','earliest_cr_line','last_pymnt_d','last_credit_pull_d']\n",
                "\n",
                "#get all numeric features\n",
                "numeric_features = make_column_selector(dtype_include =np.number)(X)\n",
                "#we get the ones with months (needs special treatment) and then all others (which are not in our drop selection)\n",
                "numeric_mths_features = [ f for f in numeric_features if ('mths_since_' in f and f not in columns_to_drop)]\n",
                "numeric_rem_features  = [ f for f in numeric_features if (f not in numeric_mths_features and f not in columns_to_drop) ]\n",
                "\n",
                "#our transformer for the date features\n",
                "date_transformer = Pipeline(steps=[\n",
                "    ('formatter', DateFormatter()),\n",
                "    ('imputer', SimpleImputer(strategy='median', missing_values = np.nan)),\n",
                "    ('scaler', StandardScaler())\n",
                "    ]\n",
                ")\n",
                "\n",
                "#our generic numeric transformer\n",
                "numeric_transformer = Pipeline(steps=[\n",
                "    ('imputer', SimpleImputer(strategy='median', missing_values = np.nan)),\n",
                "    ('scaler', StandardScaler())])\n",
                "\n",
                "#our transformer for the months column\n",
                "numeric_mths_transformer = Pipeline(steps=[\n",
                "    ('imputer', SimpleImputer(strategy='constant', missing_values = np.nan, fill_value = 100)),\n",
                "    ('scaler', StandardScaler())])\n",
                "\n",
                "#our ordinals, which are related to each other\n",
                "ordinal_features = ['term','emp_length','sub_grade','debt_settlement_flag','hardship_flag','pymnt_plan']\n",
                "ordinal_transformer = Pipeline(steps = [\n",
                "    ('imputer', SimpleImputer(strategy='constant', missing_values = np.nan, fill_value = 'None')),\n",
                "    ('encoder', OrdinalEncoder() ),\n",
                "    ('scaler', StandardScaler())\n",
                "])\n",
                "\n",
                "#a special transformer for employee title to reduce the number of sparse features it creates\n",
                "emp_title_feature = ['emp_title']\n",
                "emp_title_transformer = Pipeline( steps = [\n",
                "    ('imputer1', EmpTitleImputer()),\n",
                "    ('imputer2', SimpleImputer(strategy='constant', missing_values = np.nan, fill_value = 'None')),\n",
                "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
                "])\n",
                "\n",
                "# our general hot key encoder for categorical features\n",
                "categorical_features = [ f for f in make_column_selector(dtype_include='object')(X) if (f not in ordinal_features and f not in columns_to_drop and f not in emp_title_feature) ]\n",
                "categorical_transformer = Pipeline( steps = [\n",
                "    ('imputer', SimpleImputer(strategy='constant', missing_values = np.nan, fill_value = 'None')),\n",
                "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
                "])\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Two processors are set up, a full one, and a simple one to evaluate the effects of the processing"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Full processor including date conversion, months to high values, ordinal encoding, and truncating the employee title categories\n",
                "preprocessor = ColumnTransformer(\n",
                "    transformers=[\n",
                "        ('date', date_transformer, date_features),\n",
                "        ('num', numeric_transformer, numeric_rem_features),\n",
                "        ('num_mths', numeric_transformer, numeric_mths_features),\n",
                "        ('emp_title', emp_title_transformer, emp_title_feature),\n",
                "        ('ordinal', ordinal_transformer, ordinal_features),\n",
                "        ('cat', categorical_transformer, categorical_features)]\n",
                "        )\n",
                "\n",
                "# Simpe processor converting nan to mean for numeric and hot key encoding all categorical\n",
                "simple_preprocessor = ColumnTransformer(\n",
                "    transformers=[\n",
                "        ('num', numeric_transformer, numeric_features),\n",
                "        ('cat', categorical_transformer, categorical_features + ordinal_features + emp_title_feature)]\n",
                "        )\n",
                "\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Do a train test split of 0.2 (could also do cross-validation here but would take a bit longer for the differnet models, I'll do this for hyperparameter tuning)"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
                "                                                    random_state=0)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Just to check out the size of the data after our column transformers, and perform a quick unit test here to make sure that we're not removing any columns we shouldn't be in the processing stage (note this has to be done after the fit transform)"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "output_frame = preprocessor.fit_transform(X_train)\n",
                "output_simple_frame = simple_preprocessor.fit_transform(X_train)\n",
                "print(\"The preprocessing step has reduced the feature size from %i to %i\"%(output_simple_frame.shape[1], output_frame.shape[1]))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "\n",
                "transformer_dropped = [X_train.columns.to_list()[i] for i in preprocessor.transformers_[-1][-1]]\n",
                "transformer_dropped.sort()\n",
                "columns_to_drop.sort()\n",
                "if transformer_dropped != columns_to_drop:\n",
                "    print('Columns dropped by transformer but not in drop list', [b for b in transformer_dropped if b not in columns_to_drop])\n",
                "    print('Columns in drop list but not dropped by transformer', [b for b in columns_to_drop if b not in transformer_dropped])\n",
                "    raise RuntimeWarning('The requested variables to drop and the transformer dropped variables are not the same. Check for consistency')\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Linear Regression\n",
                "First model to try is linear regression. For a test, we'll do this with the naive simple preprocessing first, then with the full preprocessing to seee the difference. The model here is based on the $R^2$ score and the mean squared error score is also included as it's most relevant for our test sample"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "\n",
                "# Append linear regression to the simple preprocessing pipeline.\n",
                "clf_linear_simple = Pipeline(steps=[('preprocessor', simple_preprocessor),\n",
                "                      ('regression', LinearRegression())])\n",
                "\n",
                "\n",
                "clf_linear_simple.fit(X_train, y_train)\n",
                "print(\"model score: %.3f\" % clf_linear_simple.score(X_test, y_test))\n",
                "print(\"root mean squared error (test)  : %.3f\" % mean_squared_error( y_test, clf_linear_simple.predict(X_test)))\n",
                "print(\"root mean squared error (train) : %.3f\" % mean_squared_error( y_train, clf_linear_simple.predict(X_train)))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "\n",
                "# Append linear regression to the full preprocessing pipeline.\n",
                "clf_linear = Pipeline(steps=[('preprocessor', preprocessor),\n",
                "                      ('regression', LinearRegression())])\n",
                "\n",
                "\n",
                "clf_linear.fit(X_train, y_train)\n",
                "print(\"model score: %.3f\" % clf_linear.score(X_test, y_test))\n",
                "print(\"root mean squared error (test)  : %.3f\" % mean_squared_error( y_test, clf_linear.predict(X_test)))\n",
                "print(\"root mean squared error (train) : %.3f\" % mean_squared_error( y_test, clf_linear.predict(X_train)))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Clearly the full preprocessing performs better here, and also trains much quicker because it has significantly less features to deal with. However, I can still likely do better with different models. I try three specific models, Lasso Regression, KNN Regression, and Gradient Boosted Decision Tree Regression."
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Lasso Regression"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Append Lasso regression to the preprocessing pipeline.\n",
                "clf_lasso = Pipeline(steps=[('preprocessor', preprocessor),\n",
                "                      ('regression', Lasso())])\n",
                "\n",
                "clf_lasso.fit(X_train, y_train)\n",
                "print(\"model score: %.3f\" % clf_lasso.score(X_test, y_test))\n",
                "print(\"root mean squared error (test)  : %.3f\" % math.sqrt(mean_squared_error( y_test, clf_lasso.predict(X_test))))\n",
                "print(\"root mean squared error (train) : %.3f\" % math.sqrt(mean_squared_error( y_train, clf_lasso.predict(X_train))))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# KNN Regression"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Append KNN regression to the preprocessing pipeline.\n",
                "clf_knn = Pipeline(steps=[('preprocessor', preprocessor),\n",
                "                      ('regression', KNeighborsRegressor())])\n",
                "\n",
                "clf_knn.fit(X_train, y_train)\n",
                "print(\"model score: %.3f\" % clf_knn.score(X_test, y_test))\n",
                "knn_test_predictions = clf_knn.predict(X_test)\n",
                "knn_train_predictions = clf_knn.predict(X_train)\n",
                "print(\"root mean squared error (test)  : %.3f\" % math.sqrt(mean_squared_error( y_test, test_predictions)))\n",
                "print(\"root mean squared error (train) : %.3f\" % math.sqrt(mean_squared_error( y_train, train_predictions)))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Gradient Boosting Regression"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "\n",
                "# Append gradient boosting regression to the preprocessing pipeline.\n",
                "clf_gradboost = Pipeline(steps=[('preprocessor', preprocessor),\n",
                "                      ('regression', GradientBoostingRegressor())])\n",
                "\n",
                "clf_gradboost.fit(X_train, y_train)\n",
                "print(\"model score: %.3f\" % clf_gradboost.score(X_test, y_test))\n",
                "print(\"root mean squared error (test)  : %.3f\" % math.sqrt(mean_squared_error( y_test, clf_gradboost.predict(X_test))))\n",
                "print(\"root mean squared error (train) : %.3f\" % math.sqrt(mean_squared_error( y_train, clf_gradboost.predict(X_train))))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Comparing Models\n",
                "\n",
                "The main mode of comparison is the model score listed above, but it's worth also visualising comparing them. I show just two different types of comparisons, a scatter of the predicted versus true for the linear and gradient boosted regression results, and then also histograms of the error in the prediction, the width of which determined the accuracy of the algorithm."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "fig,(ax1,ax2,ax3) = plt.subplots(1,3, figsize = (30,10))\n",
                "ax1.scatter(y_test, clf_linear.predict(X_test), color='blue', alpha=0.2,label='Linear Regression')\n",
                "ax1.scatter(y_test, clf_gradboost.predict(X_test), color='green', alpha=0.2, label='Gradient Boosting Regression')\n",
                "\n",
                "ax2.hist( y_test.values.flatten() - clf_gradboost.predict(X_test).flatten(), np.linspace(-5,5,100), facecolor='green'  , label='Gradient Boosting Regression', histtype = 'stepfilled')\n",
                "ax2.hist( y_test.values.flatten() - clf_linear.predict(X_test).flatten()   , np.linspace(-5,5,100), edgecolor='blue'   , label='Linear Regression', facecolor = \"None\", histtype = 'stepfilled')\n",
                "\n",
                "ax3.hist( y_test.values.flatten() - clf_lasso.predict(X_test).flatten()    , np.linspace(-5,5,100), edgecolor='orange' , label='Lasso Regression', facecolor = \"None\", histtype = 'stepfilled')\n",
                "ax3.hist( y_test.values.flatten() - knn_test_predictions.flatten()    , np.linspace(-5,5,100), edgecolor='purple' , label='KNN Regression', facecolor = \"None\", histtype = 'stepfilled')\n",
                "\n",
                "\n",
                "ax1.set_xlabel('True Value')\n",
                "ax1.set_ylabel('Predicted Value')\n",
                "ax1.legend()\n",
                "ax2.legend()\n",
                "ax2.set_xlabel('True - Predicted')\n",
                "ax2.set_ylabel('Number of Entries')\n",
                "ax3.legend()\n",
                "ax3.set_xlabel('True - Predicted')\n",
                "ax3.set_ylabel('Number of Entries')\n",
                "plt.show()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Hyperparameter Tuning\n",
                "\n",
                "The gradient boosting performs the best out of the models chosen on all counts. It has the best room mean squared error and this is consistent between the test and training samples, indicating there is none of the overfitting which is present in the other models. It's now worth spending some time trying to tune the hyperparameters of the algorithm to see if I can optimise a bit further. The number of hyperparameters possible is large, so I'll just pick a few, and vary one at a time."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "default_gradboost_params = {\n",
                "    'max_features' : None,\n",
                "    'loss' : 'ls',\n",
                "    'learning_rate': 0.1,\n",
                "    'n_estimators': 100,\n",
                "    'subsample' : 1.0,\n",
                "    'criterion' : 'friedman_mse',\n",
                "    'tol' : 1e-4}\n",
                "\n",
                "param_grid = [\n",
                "    {'regression__max_features' : [None, 'sqrt', 'log2']},\n",
                "    {'regression__loss' : ['ls', 'lad']},\n",
                "    {'regression__learning_rate': [0.1,1.0]},\n",
                "    {'regression__n_estimators' : [100,200]}\n",
                "]\n",
                "search = GridSearchCV(clf_gradboost, param_grid, cv=2).fit(X, y)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Final model training\n",
                "\n",
                "Do a final train of the model using the parameters from the hyperparameter tuning"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "best_params = search.best_estimator_.named_steps['regression'].get_params()\n",
                "print(best_params)\n",
                "clf_gradboost.named_steps['regression'].set_params(**best_params)\n",
                "clf_gradboost.fit(X_train, y_train)\n",
                "print(\"model score: %.3f\" % clf_gradboost.score(X_test, y_test))\n",
                "print(\"root mean squared error (test)  : %.3f\" % math.sqrt(mean_squared_error( y_test, clf_gradboost.predict(X_test))))\n",
                "print(\"root mean squared error (train) : %.3f\" % math.sqrt(mean_squared_error( y_train, clf_gradboost.predict(X_train))))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Evaluation on separate test sample\n",
                "\n",
                "The final step is to do the evaluation on the final test sample! We expect a mean-squared error of 0.273% on this. The scikitlearn pipeline allows us to load and process this sample in a straightforward manner."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "dataframe_test = pd.read_csv('DataScienceAssignment/DataScienceAssignment_test.csv')\n",
                "dataframe_test.drop_duplicates(inplace=True)\n",
                "dataframe_test.set_index('id')\n",
                "\n",
                "test_predictions = clf_gradboost.predict(dataframe_test)\n",
                "\n",
                "final_test_preds = pd.DataFrame.from_dict({'id' : dataframe_test['id'], 'int_rate' : test_predictions})\n",
                "final_test_preds.to_csv('DataScienceAssignment/DataScienceAssignment_test_predictions.csv')"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Outlook\n",
                "\n",
                "There are a number of things that could still be done with the regression model\n",
                "1. Understanding a bit better some of the data. Can employee titles be grouped a bit better? Is the missing data really just missing data or does it indicate a certain value (i.e. inf. for the mths_since columns)? If the former, should it be penalised in final evaluation?\n",
                "2. Even more would be interesting to interface with xgboost and to try neural network regression which may capture some more of the complexity.\n",
                "3. Unit tests to ensure the processing and predictions behave as expected in training and when applied to any dataset. In particula validating the transforms which can be a bit tricky in the column transformer framework.\n",
                "4. Pickle final algorithm for deployment"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.9.5",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.9.5 64-bit"
        },
        "interpreter": {
            "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}